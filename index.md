---
layout: default
---
Course materials for the
[Auditory Computing](https://www.ucl.ac.uk/module-catalogue/modules/auditory-computing-COMP0161)
module at [UCL Computer Science](https://www.ucl.ac.uk/computer-science/), for delivery in
Term 2 (January-March 2024).

## Tutorial Lab Sessions: Generating Music with Deep Learning

Please bring your laptop and suitable headphones/earphones.

The practical exercises are run from within a web browser using
[Google Colab](https://colab.research.google.com/). You will need
a (free) Google account to use this service.

To launch each notebook, click the corresponding "Open in Colab" badge.
You might receive a warning that the notebook was not provided by
Google â€” which is of course true, these notebooks were written by
Matthew Caldwell for COMP0161. They will not, in fact, attempt to do
anything untoward with your data, but you will have to take my word for
that.


* **Lab 1: Data** (1 Feb 2024)
    * In this lab we build a text-encoded dataset of classical piano music.
<!--    * [dev notebook](https://colab.research.google.com/drive/1vkN__9VWqz4SaEgQDDvWEWqKANHk9wnl) (temporary) -->
    * [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comp0161/colab/blob/main/COMP0161_lab1.ipynb)
* **Lab 2: Learning** (22 Feb 2024)
    * In this lab we use the compiled dataset from Lab 1 to train a small GPT-style model
      to generate music in a similar style.
<!--    * [dev notebook](https://colab.research.google.com/drive/1tfWhkgOYNF-KGZZu4Fc-nrBs1NEgC1m7) (temporary) -->
    * [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comp0161/colab/blob/main/COMP0161_lab2.ipynb)
* **Lab 3: Synthesis & Effects** (29 Feb 2024)
    * In this final session we tweak the instrument sound and apply
      a variety of audio effects to the music generated in Lab 2.
<!--    * [dev notebook](https://colab.research.google.com/drive/1x23bALrzIQeRxzAyyfcz44b3vdyLlK9b) (temporary) -->
    * [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comp0161/colab/blob/main/COMP0161_lab3.ipynb)


## Links & Resources

* [Auditory Modeling Toolbox](https://amtoolbox.org)
* [Al Bregman's Auditory Scene Analysis](https://webpages.mcgill.ca/staff/Group2/abregm1/web/)
* [Dannenberg Introduction to Music Concepts](https://www.cs.cmu.edu/~music/cmsip/readings/music-theory.htm)
* [The Hearing Garden](https://www.hz-ol.de/en/listening-garden.html)
* [Pure Data (Pd)](https://puredata.info/)
    * [Tutorial: Programming Electronic Music in Pd](http://pd-tutorial.com/english/index.html)
    * [Purr Data](https://www.purrdata.net)
* [SuperCollider](https://supercollider.github.io)
* [Csound](https://csound.com)
* [Cmajor](https://cmajor.dev)
* [MuseScore](https://musescore.org)
* [LilyPond](https://lilypond.org)
* [FluidSynth](https://www.fluidsynth.org)
* [VCV Rack](https://vcvrack.com/Rack)
* [Music21](https://web.mit.edu/music21/)
* [Pedalboard](https://github.com/spotify/pedalboard)
