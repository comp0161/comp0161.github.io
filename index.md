---
layout: default
---
Course materials for the
[Auditory Computing](https://www.ucl.ac.uk/module-catalogue/modules/auditory-computing-COMP0161)
module at [UCL Computer Science](https://www.ucl.ac.uk/computer-science/), for delivery in
Term 2 (January-March 2025).

## Tutorial Lab Sessions

**Please bring your laptop and suitable headphones/earphones!**

A provisional schedule is shown below — note that
the topics are only indicative and will almost certainly change
as we go along. Links to any associated content or requirements
for each week will be added as necessary.
Tutorial code will generally be found in the
[tutorials repo](https://github.com/comp0161/tutorials).

* Lab 1: **Basic Acoustics & Auditory Perception** (16 Jan 2025)
    * [`lab01.py`](https://github.com/comp0161/tutorials/blob/main/lab01.py)
      is a script for some basic sound generation and playback using
      the [simpleaudio](https://simpleaudio.readthedocs.io/en/latest/)
      python package. Provided in case anyone is interested,
      but note that **you do not need to run this yourself**.
    * The tutorial makes use of some free audio metering plug-ins. Again, **you
      do not need these** for anything on the module, but they can be useful for
      visualising audio data. Links included for completeness, feel free to ignore.
        * [Wave Observer](https://pressplay-music.com/wave-observer/) (oscilloscope)
        * [Voxengo SPAN](https://www.voxengo.com/product/span/) (spectrum analyser)
        * [TB Spectrogram](https://www.toneboosters.com/tb_spectrogram_v1.html) (spectrogram & spectrum analyser)
* Lab 2: **Signal Detection & Psychophysics** (23 Jan 2025)
    * During the lab you will act as experimental subjects in some basic psychophysics
      experiments that run in your browser. (You will definitely need headphones for these.)
        * [Calibration](experiments/calibration/?home=/index.html)
        * [Pitch sensitivity](experiments/pitch/?home=/index.html)
        * [Detection](experiments/freqlevel/?home=/index.html)
* Lab 3: **Digital Signal Processing** (30 Jan 2025)
* Lab 4: **Auditory Scene Analysis** (6 Feb 2025)
* Lab 5: **Speech Recognition** (13 Feb 2025)
* Lab 6: **Space & Localisation** (27 Feb 2025)
* Lab 7: **Pitch & Texture** (6 Mar 2025)
* Lab 8: **Sonification** (13 Mar 2025)
* Lab 9: **Generating Music with Deep Learning** (20 Mar 2025)

<!--
The practical exercises are run from within a web browser using
[Google Colab](https://colab.research.google.com/). You will need
a (free) Google account to use this service.

To launch each notebook, click the corresponding "Open in Colab" badge.
You might receive a warning that the notebook was not provided by
Google — which is of course true, these notebooks were written by
Matthew Caldwell for COMP0161. They will not, in fact, attempt to do
anything untoward with your data, but you will have to take my word for
that.


* **Lab 1: Data** (1 Feb 2024)
    * In this lab we build a text-encoded dataset of classical piano music.
    * [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comp0161/colab/blob/main/COMP0161_lab1.ipynb)
* **Lab 2: Learning** (22 Feb 2024)
    * In this lab we use the compiled dataset from Lab 1 to train a small GPT-style model
      to generate music in a similar style.
    * [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comp0161/colab/blob/main/COMP0161_lab2.ipynb)
* **Lab 3: Synthesis & Effects** (29 Feb 2024)
    * In this final session we tweak the instrument sound and apply
      a variety of audio effects to the music generated in Lab 2.
    * [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comp0161/colab/blob/main/COMP0161_lab3.ipynb)
-->


## Links & Resources

Some of these may be discussed or used in lectures or practicals, others
are purely for interest. At some point they might get organised along
such lines, but in the meantime feel free to browse around.

* [Auditory Modeling Toolbox](https://amtoolbox.org)
* [Al Bregman's Auditory Scene Analysis](https://webpages.mcgill.ca/staff/Group2/abregm1/web/)
* [Dannenberg Introduction to Music Concepts](https://www.cs.cmu.edu/~music/cmp/archives/cmsip/readings/music-theory.htm)
* [The Hearing Garden](https://www.hz-ol.de/en/listening-garden.html)
* [Pure Data (Pd)](https://puredata.info/)
    * [Tutorial: Programming Electronic Music in Pd](http://pd-tutorial.com/english/index.html)
    * [Purr Data](https://www.purrdata.net)
* [SuperCollider](https://supercollider.github.io)
    * [TidalCycles](https://tidalcycles.org)
* [Csound](https://csound.com)
* [Cmajor](https://cmajor.dev)
* [MuseScore](https://musescore.org)
* [LilyPond](https://lilypond.org)
* [FluidSynth](https://www.fluidsynth.org)
* [VCV Rack](https://vcvrack.com/Rack)
* [Music21](https://www.music21.org/music21docs/)
* [Pedalboard](https://github.com/spotify/pedalboard)
* [Strudel](https://strudel.cc)
* [Estuary](https://estuary.mcmaster.ca)
